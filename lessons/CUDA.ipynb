{
 "metadata": {
  "name": "",
  "signature": "sha256:9c390b921f2ad56e43d7adbf2c298792bcde729f6c937046cb393bf5e85c2983"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### CUDA Python\n",
      "\n",
      "Most of the time, it is sufficient to use the high-level decorators `jit`, `autojit`, `vectorize` and `guvectorize` for running functoins on the GPU, but sometimes, explicit control is necessary. This is provided by the \n",
      "\n",
      "Low level Python code using the numbapro.cuda module is similar to CUDA C, and will compile to the same machine code, but with the benefits of integerating into Python for use of numpy arrays, convenient I/O, graphics etc.\n",
      "\n",
      "Optionally, CUDA Python can provide\n",
      "\n",
      "* Automatic memory transfer\n",
      "    * NumPy arrays are automatically transferred\n",
      "    * CPU -> GPU\n",
      "    * GPU -> CPU\n",
      "* Automatic work scheduling\n",
      "    * The work is distributed the across all threads on the GPU\n",
      "    * The GPU hardware handles the scheduling\n",
      "* Automatic GPU memory management\n",
      "    * GPU memory is tied to object lifetime\n",
      "    * freed automatically\n",
      "   \n",
      "but these can be over-riden with explicit control instructions if desired. [Source](http://nbviewer.ipython.org/github/ContinuumIO/numbapro-examples/blob/master/webinars/2014_06_17/intro_to_gpu_python.ipynb)\n",
      "\n",
      "Python CUDA also provides syntactic sugar for obtaining thread identity. For example,\n",
      "\n",
      "```python\n",
      "tx = cuda.threadIdx.x\n",
      "ty = cuda.threadIdx.y\n",
      "bx = cuda.blockIdx.x\n",
      "by = cuda.blockIdx.y\n",
      "bw = cuda.blockDim.x\n",
      "bh = cuda.blockDim.y\n",
      "x = tx + bx * bw\n",
      "y = ty + by * bh\n",
      "array[x, y] = something(x, y)\n",
      "``` \n",
      "can be abbreivated to\n",
      "```python\n",
      "x, y = cuda.grid(2)\n",
      "array[x, y] = something(x, y)\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numbapro import cuda, vectorize, guvectorize\n",
      "from numbapro import void, int64, float32, float64\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gpu = cuda.get_current_device()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Vector addition with the `vectorize` decorator"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@vectorize(['int64(int64, int64)', \n",
      "            'float32(float32, float32)',\n",
      "            'float64(float64, float64)'], \n",
      "           target='gpu')\n",
      "def cu_add(a, b):\n",
      "    return a + b"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = 100\n",
      "a = np.arange(n, dtype=np.float32)\n",
      "b = np.arange(n, dtype=np.float32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = cu_add(a, b)\n",
      "print c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[   0.    2.    4.    6.    8.   10.   12.   14.   16.   18.   20.   22.\n",
        "   24.   26.   28.   30.   32.   34.   36.   38.   40.   42.   44.   46.\n",
        "   48.   50.   52.   54.   56.   58.   60.   62.   64.   66.   68.   70.\n",
        "   72.   74.   76.   78.   80.   82.   84.   86.   88.   90.   92.   94.\n",
        "   96.   98.  100.  102.  104.  106.  108.  110.  112.  114.  116.  118.\n",
        "  120.  122.  124.  126.  128.  130.  132.  134.  136.  138.  140.  142.\n",
        "  144.  146.  148.  150.  152.  154.  156.  158.  160.  162.  164.  166.\n",
        "  168.  170.  172.  174.  176.  178.  180.  182.  184.  186.  188.  190.\n",
        "  192.  194.  196.  198.]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Switching execution target\n",
      "\n",
      "One advantage of the high-level vectorize decorator is that the funciton code will run without any change on a single core, multiple cores or GPU by simply chaning the target. This can be used to run the apprropriate code depending on problem type and size, or as a fallback on machines that lack a GPU."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run in parallel on mulitple CPU cores by changing target\n",
      "@vectorize(['int64(int64, int64)', \n",
      "            'float64(float32, float32)',\n",
      "            'float64(float64, float64)'], \n",
      "           target='parallel')\n",
      "def mc_add(a, b):\n",
      "    return a + b\n",
      "\n",
      "mc_add(a, b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "array([   0.,    2.,    4.,    6.,    8.,   10.,   12.,   14.,   16.,\n",
        "         18.,   20.,   22.,   24.,   26.,   28.,   30.,   32.,   34.,\n",
        "         36.,   38.,   40.,   42.,   44.,   46.,   48.,   50.,   52.,\n",
        "         54.,   56.,   58.,   60.,   62.,   64.,   66.,   68.,   70.,\n",
        "         72.,   74.,   76.,   78.,   80.,   82.,   84.,   86.,   88.,\n",
        "         90.,   92.,   94.,   96.,   98.,  100.,  102.,  104.,  106.,\n",
        "        108.,  110.,  112.,  114.,  116.,  118.,  120.,  122.,  124.,\n",
        "        126.,  128.,  130.,  132.,  134.,  136.,  138.,  140.,  142.,\n",
        "        144.,  146.,  148.,  150.,  152.,  154.,  156.,  158.,  160.,\n",
        "        162.,  164.,  166.,  168.,  170.,  172.,  174.,  176.,  178.,\n",
        "        180.,  182.,  184.,  186.,  188.,  190.,  192.,  194.,  196.,  198.])"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Vector addition with CUDA Python\n",
      "\n",
      "Ignoring minor syntactic differneces and absence of boilerplate for memory managemnt, these CUDA Python functions should be familiar to CUDA C programmers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@cuda.jit('void(float32[:], float32[:], float32[:])')\n",
      "def cu_add(a, b, c):\n",
      "    i = cuda.grid(1)\n",
      "\n",
      "    if i  > c.size:\n",
      "        return\n",
      "    c[i] = a[i] + b[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tpb = gpu.WARP_SIZE\n",
      "bpg = int(np.ceil(float(n)/nthreads))\n",
      "print 'Blocks per grid:', bpg\n",
      "print 'Threads per block', tpb\n",
      "\n",
      "c = np.empty_like(a)\n",
      "cu_add[bpg, tpb](a, b, c)\n",
      "print c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Blocks per grid: 4\n",
        "Threads per block 32\n",
        "[[  1.60478759e+00   4.56136663e-41   1.29999459e-36 ...,   3.39841957e+01\n",
        "    3.35781860e+01   3.07690029e+01]\n",
        " [  9.41525936e-01   3.22783546e+01   3.34166641e+01 ...,   3.45216675e+01\n",
        "    3.37318153e+01   2.97101288e+01]\n",
        " [  4.51084495e-01   2.96663380e+01   3.10288658e+01 ...,   3.13128777e+01\n",
        "    3.03996162e+01   2.74207344e+01]\n",
        " ..., \n",
        " [  3.10007591e+01   2.82686005e+01   2.86496010e+01 ...,   3.13911858e+01\n",
        "    3.28465118e+01   2.82789288e+01]\n",
        " [  3.33143654e+01   2.91543407e+01   3.08795128e+01 ...,   3.01875916e+01\n",
        "    3.24928703e+01   2.76480503e+01]\n",
        " [  3.20372009e+01   2.94373665e+01   3.08756065e+01 ...,   2.98975697e+01\n",
        "    3.28550415e+01   2.82880650e+01]]\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Navie matrix multiply"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image \n",
      "Image(url=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/matrix-multiplication-without-shared-memory.png\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<img src=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/matrix-multiplication-without-shared-memory.png\"/>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "<IPython.core.display.Image at 0x7f27232fce10>"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x1 = np.random.random((2,4))\n",
      "x2 = np.random.random((4,3))\n",
      "np.dot(x1, x2).shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "(2, 3)"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@cuda.jit('void(float32[:,:], float32[:,:], float32[:,:], int32)')\n",
      "def cu_matmul(a, b, c, n):\n",
      "    x, y = cuda.grid(2)\n",
      "    \n",
      "    if (x >= n) or (y >= n):\n",
      "        return\n",
      "    \n",
      "    c[y, x] = 0\n",
      "    for i in range(n):\n",
      "        c[y, x] +=  a[y, i] * b[i, x]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tpb = gpu.WARP_SIZE\n",
      "bpg = 4\n",
      "print 'Blocks per grid:', tpb\n",
      "print 'Threads per block', bpg\n",
      "\n",
      "n = bpg * tpb\n",
      "\n",
      "a = np.random.random((n, n)).astype(np.float32)\n",
      "b = np.random.random((n, n)).astype(np.float32)\n",
      "c = np.empty((n, n), dtype=np.float32)\n",
      "cu_matmul[(bpg, bpg), (tpb, tpb)](a, b, c, n)\n",
      "assert(np.allclose(np.dot(a, b), c))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Blocks per grid: 32\n",
        "Threads per block 4\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    }
   ],
   "metadata": {}
  }
 ]
}