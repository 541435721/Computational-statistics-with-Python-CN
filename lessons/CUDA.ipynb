{
 "metadata": {
  "name": "",
  "signature": "sha256:f98c6679aec24ce0791a6b738f2898ac08ef28db31a16b22142d086f4a7028c4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "from IPython.display import Image "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### CUDA Python\n",
      "\n",
      "Most of the time, it is sufficient to use the high-level decorators `jit`, `autojit`, `vectorize` and `guvectorize` for running functoins on the GPU, but sometimes, explicit control is necessary. This is provided by the \n",
      "\n",
      "Low level Python code using the numbapro.cuda module is similar to CUDA C, and will compile to the same machine code, but with the benefits of integerating into Python for use of numpy arrays, convenient I/O, graphics etc.\n",
      "\n",
      "Optionally, CUDA Python can provide\n",
      "\n",
      "* Automatic memory transfer\n",
      "    * NumPy arrays are automatically transferred\n",
      "    * CPU -> GPU\n",
      "    * GPU -> CPU\n",
      "* Automatic work scheduling\n",
      "    * The work is distributed the across all threads on the GPU\n",
      "    * The GPU hardware handles the scheduling\n",
      "* Automatic GPU memory management\n",
      "    * GPU memory is tied to object lifetime\n",
      "    * freed automatically\n",
      "   \n",
      "but these can be over-riden with explicit control instructions if desired. [Source](http://nbviewer.ipython.org/github/ContinuumIO/numbapro-examples/blob/master/webinars/2014_06_17/intro_to_gpu_python.ipynb)\n",
      "\n",
      "Python CUDA also provides syntactic sugar for obtaining thread identity. For example,\n",
      "\n",
      "```python\n",
      "tx = cuda.threadIdx.x\n",
      "ty = cuda.threadIdx.y\n",
      "bx = cuda.blockIdx.x\n",
      "by = cuda.blockIdx.y\n",
      "bw = cuda.blockDim.x\n",
      "bh = cuda.blockDim.y\n",
      "x = tx + bx * bw\n",
      "y = ty + by * bh\n",
      "array[x, y] = something(x, y)\n",
      "``` \n",
      "can be abbreivated to\n",
      "```python\n",
      "x, y = cuda.grid(2)\n",
      "array[x, y] = something(x, y)\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Image(url=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/grid-of-thread-blocks.png\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<img src=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/grid-of-thread-blocks.png\"/>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "<IPython.core.display.Image at 0x7fefd81cd610>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numbapro import cuda, vectorize, guvectorize\n",
      "from numbapro import void, int64, float32, float64\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gpu = cuda.get_current_device()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Vector addition with the `vectorize` decorator"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@vectorize(['int64(int64, int64)', \n",
      "            'float32(float32, float32)',\n",
      "            'float64(float64, float64)'], \n",
      "           target='gpu')\n",
      "def cu_add(a, b):\n",
      "    return a + b"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = 100\n",
      "a = np.arange(n, dtype=np.float32)\n",
      "b = np.arange(n, dtype=np.float32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = cu_add(a, b)\n",
      "print c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[   0.    2.    4.    6.    8.   10.   12.   14.   16.   18.   20.   22.\n",
        "   24.   26.   28.   30.   32.   34.   36.   38.   40.   42.   44.   46.\n",
        "   48.   50.   52.   54.   56.   58.   60.   62.   64.   66.   68.   70.\n",
        "   72.   74.   76.   78.   80.   82.   84.   86.   88.   90.   92.   94.\n",
        "   96.   98.  100.  102.  104.  106.  108.  110.  112.  114.  116.  118.\n",
        "  120.  122.  124.  126.  128.  130.  132.  134.  136.  138.  140.  142.\n",
        "  144.  146.  148.  150.  152.  154.  156.  158.  160.  162.  164.  166.\n",
        "  168.  170.  172.  174.  176.  178.  180.  182.  184.  186.  188.  190.\n",
        "  192.  194.  196.  198.]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Switching execution target\n",
      "\n",
      "One advantage of the high-level vectorize decorator is that the funciton code will run without any change on a single core, multiple cores or GPU by simply chaning the target. This can be used to run the apprropriate code depending on problem type and size, or as a fallback on machines that lack a GPU."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run in parallel on mulitple CPU cores by changing target\n",
      "@vectorize(['int64(int64, int64)', \n",
      "            'float64(float32, float32)',\n",
      "            'float64(float64, float64)'], \n",
      "           target='parallel')\n",
      "def mc_add(a, b):\n",
      "    return a + b\n",
      "\n",
      "mc_add(a, b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "array([   0.,    2.,    4.,    6.,    8.,   10.,   12.,   14.,   16.,\n",
        "         18.,   20.,   22.,   24.,   26.,   28.,   30.,   32.,   34.,\n",
        "         36.,   38.,   40.,   42.,   44.,   46.,   48.,   50.,   52.,\n",
        "         54.,   56.,   58.,   60.,   62.,   64.,   66.,   68.,   70.,\n",
        "         72.,   74.,   76.,   78.,   80.,   82.,   84.,   86.,   88.,\n",
        "         90.,   92.,   94.,   96.,   98.,  100.,  102.,  104.,  106.,\n",
        "        108.,  110.,  112.,  114.,  116.,  118.,  120.,  122.,  124.,\n",
        "        126.,  128.,  130.,  132.,  134.,  136.,  138.,  140.,  142.,\n",
        "        144.,  146.,  148.,  150.,  152.,  154.,  156.,  158.,  160.,\n",
        "        162.,  164.,  166.,  168.,  170.,  172.,  174.,  176.,  178.,\n",
        "        180.,  182.,  184.,  186.,  188.,  190.,  192.,  194.,  196.,  198.])"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Vector addition with CUDA Python\n",
      "\n",
      "Ignoring minor syntactic differneces and absence of boilerplate for memory managemnt, these CUDA Python functions should be familiar to CUDA C programmers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@cuda.jit('void(float32[:], float32[:], float32[:])')\n",
      "def cu_add(a, b, c):\n",
      "    i = cuda.grid(1)\n",
      "\n",
      "    if i  > c.size:\n",
      "        return\n",
      "    c[i] = a[i] + b[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tpb = gpu.WARP_SIZE\n",
      "bpg = int(np.ceil(float(n)/tpb))\n",
      "print 'Blocks per grid:', bpg\n",
      "print 'Threads per block', tpb\n",
      "\n",
      "c = np.empty_like(a)\n",
      "cu_add[bpg, tpb](a, b, c)\n",
      "print c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Blocks per grid: 4\n",
        "Threads per block 32\n",
        "[   0.    2.    4.    6.    8.   10.   12.   14.   16.   18.   20.   22.\n",
        "   24.   26.   28.   30.   32.   34.   36.   38.   40.   42.   44.   46.\n",
        "   48.   50.   52.   54.   56.   58.   60.   62.   64.   66.   68.   70.\n",
        "   72.   74.   76.   78.   80.   82.   84.   86.   88.   90.   92.   94.\n",
        "   96.   98.  100.  102.  104.  106.  108.  110.  112.  114.  116.  118.\n",
        "  120.  122.  124.  126.  128.  130.  132.  134.  136.  138.  140.  142.\n",
        "  144.  146.  148.  150.  152.  154.  156.  158.  160.  162.  164.  166.\n",
        "  168.  170.  172.  174.  176.  178.  180.  182.  184.  186.  188.  190.\n",
        "  192.  194.  196.  198.]\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Navie matrix multiply\n",
      "\n",
      "* Grids, blocks and threads\n",
      "* Maximum size of block is 512 or 1024 threads, depending on GPU\n",
      "* Get around by using many blocks of threads to partition matrix computataions\n",
      "* Full matrix divided into tiles\n",
      "* See Figure below"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Image(url=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/matrix-multiplication-with-shared-memory.png\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<img src=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/matrix-multiplication-with-shared-memory.png\"/>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "<IPython.core.display.Image at 0x7fefd81cdb10>"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x1 = np.random.random((4,4))\n",
      "x2 = np.random.random((4,4))\n",
      "np.dot(x1, x2).shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "(4, 4)"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Kernel function (no shared memory)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@cuda.jit('void(float32[:,:], float32[:,:], float32[:,:], int32)')\n",
      "def cu_matmul(a, b, c, n):\n",
      "    x, y = cuda.grid(2)\n",
      "    \n",
      "    if (x >= n) or (y >= n):\n",
      "        return\n",
      "    \n",
      "    c[x, y] = 0\n",
      "    for i in range(n):\n",
      "        c[x, y] +=  a[x, i] * b[i, y]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tpb = gpu.WARP_SIZE\n",
      "n = 400\n",
      "bpg = (n+tpb-1)/tpb\n",
      "grid_dim = (bpg, bpg)\n",
      "block_dim = (tpb, tpb)\n",
      "\n",
      "A = np.random.random((n, n)).astype(np.float32)\n",
      "B = np.random.random((n, n)).astype(np.float32)\n",
      "C = np.empty((n, n), dtype=np.float32)\n",
      "cu_matmul[grid_dim, block_dim](A, B, C, n)\n",
      "assert(np.allclose(np.dot(A, B), C))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Matrix multiply with shared memory\n",
      "\n",
      "Memmory access speed\n",
      "* Local to thread\n",
      "* Shared among block of threads\n",
      "* Global (much slower than shared) \n",
      "* Host\n",
      "\n",
      "Want to push memory access as close to threads as possible. In practice, the challenge is usually to structure the program in such a way that shared mmeory use is optimized."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Image(url=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/memory-hierarchy.png\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<img src=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/memory-hierarchy.png\"/>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "<IPython.core.display.Image at 0x7fefaea0be50>"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Using shared mmeory by using tiling to exploit locality"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Image(url=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/matrix-multiplication-with-shared-memory.png\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<img src=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/matrix-multiplication-with-shared-memory.png\"/>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "<IPython.core.display.Image at 0x7fefae9ffad0>"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Kernel function (with shared memory)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tpb = gpu.WARP_SIZE\n",
      "block_dim = (tpb, tpb)\n",
      "\n",
      "@cuda.jit('void(float32[:,:], float32[:,:], float32[:,:], int32, int32, int32)')\n",
      "def cu_matmul_sm(A, B, C, n, tpb, bpg):\n",
      "    # decalre shared memory\n",
      "    sA = cuda.shared.array(shape=block_dim, dtype=float32)\n",
      "    sB = cuda.shared.array(shape=block_dim, dtype=float32)\n",
      "    \n",
      "    # we now need the thread ID within a block as well as the global thread ID\n",
      "    tx = cuda.threadIdx.x\n",
      "    ty = cuda.threadIdx.y\n",
      "    x, y = cuda.grid(2)    \n",
      "    \n",
      "    # pefort partial operations in block-szied tiles\n",
      "    # saving intermediate values in an accumulator variable\n",
      "    acc = 0.0\n",
      "    for i in range(bpg):\n",
      "        # Stage 1: Prefil shared memory with current block from matrix A and matrix B\n",
      "        sA[tx, ty] = A[x, ty + i * tpb]\n",
      "        sB[tx, ty] = B[tx + i * tpb, y]\n",
      "        \n",
      "        # Block calculations till shared mmeory is filled\n",
      "        cuda.syncthreads()\n",
      "        \n",
      "        # Stage 2: Compute partial dot product and add to accumulator\n",
      "        if x < n and y < n:\n",
      "            for j in range(tpb):\n",
      "                acc += sA[tx, j] * sB[j, ty]\n",
      "                \n",
      "        # Blcok until all threads have completed calcuaiton before next loop iteration\n",
      "        cuda.syncthreads()\n",
      "        \n",
      "    # Put accumulated dot product into output matrix\n",
      "    if x < n and y < n:\n",
      "        C[x, y] = acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = 32\n",
      "n = tpb * k # n must be multiple of tpb because shared memory is not initialized to zero\n",
      "bpg = n/tpb\n",
      "grid_dim = (bpg, bpg)\n",
      "\n",
      "A = np.random.random((n, n)).astype(np.float32)\n",
      "B = np.random.random((n, n)).astype(np.float32)\n",
      "C = np.empty((n, n), dtype=np.float32)\n",
      "cu_matmul_sm[grid_dim, block_dim](A, B, C, n, tpb, bpg)\n",
      "assert(np.allclose(np.dot(A, B), C))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Usign CUBLAS"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = 32\n",
      "n = tpb * k\n",
      "bpg = n/tpb\n",
      "grid_dim = (bpg, bpg)\n",
      "\n",
      "import numbapro.cudalib.cublas as cublas \n",
      "\n",
      "A = np.asfortranarray(np.random.random((n, n)).astype(np.float32))\n",
      "B = np.asfortranarray(np.random.random((n, n)).astype(np.float32))\n",
      "C = np.zeros_like(A, order='F')\n",
      "blas = cublas.Blas()\n",
      "blas.gemm('N', 'N', n, n, n, 1.0, A, B, 1.0, C)\n",
      "assert(np.allclose(np.dot(A, B), C))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 112
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Benchmark"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from timeit import default_timer as timer\n",
      "\n",
      "k = 150\n",
      "n = tpb * k\n",
      "bpg = n/tpb\n",
      "grid_dim = (bpg, bpg)\n",
      "\n",
      "# Prepare data on the CPU\n",
      "A = np.array(np.random.random((n, n)), dtype=np.float32)\n",
      "B = np.array(np.random.random((n, n)), dtype=np.float32)\n",
      "C = np.zeros_like(A)\n",
      "\n",
      "print \"N = %d x %d\" % (n, n)\n",
      "\n",
      "# Prepare data on the GPU\n",
      "dA = cuda.to_device(A)\n",
      "dB = cuda.to_device(B)\n",
      "dC = cuda.to_device(C) # device_array_like(A)\n",
      "\n",
      "# Time numpy version\n",
      "s = timer()\n",
      "np_ans = np.dot(A, B)\n",
      "e = timer()\n",
      "t = e - s\n",
      "\n",
      "# Time the unoptimized version\n",
      "s = timer()\n",
      "cu_matmul[grid_dim, block_dim](dA, dB, dC, n)\n",
      "cuda.synchronize()\n",
      "e = timer()\n",
      "unopt_ans = dC.copy_to_host()\n",
      "tcuda_unopt = e - s\n",
      "\n",
      "# Time the optimized version\n",
      "s = timer()\n",
      "cu_matmul_sm[grid_dim, block_dim](dA, dB, dC, n, tpb, bpg)\n",
      "cuda.synchronize()\n",
      "e = timer()\n",
      "opt_ans = dC.copy_to_host()\n",
      "tcuda_opt = e - s\n",
      "\n",
      "# Time for CuBLAS version\n",
      "s = timer()\n",
      "blas.gemm('T', 'T', n, n, n, 1.0, A, B, 1.0, C) # A, B not in fortran order so need for transpose\n",
      "e = timer()\n",
      "blas_ans = dC.copy_to_host()\n",
      "tcuda_blas = e - s\n",
      "\n",
      "print \"Using numpy.dot:\", \"%.2f\" % t, \"s\"\n",
      "print \"Without shared memory:\", \"%.2f\" % tcuda_unopt, \"s\"\n",
      "print \"With shared memory:\", \"%.2f\" % tcuda_opt, \"s\"\n",
      "print \"Using CuBLAS:\", \"%.2f\" % tcuda_blas, \"s\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert np.allclose(np_ans, unopt_ans)\n",
      "assert np.allclose(np_ans, opt_ans)\n",
      "assert np.allclose(np_ans, blas_ans)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 117
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}