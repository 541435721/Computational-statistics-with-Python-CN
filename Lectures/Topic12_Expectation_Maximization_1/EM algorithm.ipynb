{
 "metadata": {
  "name": "",
  "signature": "sha256:e3cd07081314cc0f297664a11425eafa37b3e8282ff8a76bce146a1298e1072b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "import glob\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "%precision 4\n",
      "plt.style.use('ggplot')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image\n",
      "from numpy.core.umath_tests import matrix_multiply as mm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Outline\n",
      "----\n",
      "\n",
      "- Review of Jensen's inequality\n",
      "- Concavity of log function\n",
      "- Example of coin tossing with missing informaiton to provide context\n",
      "- Derivation of EM equations\n",
      "- Illustration of EM convergence\n",
      "- Derivation of update equations of coin tossing example\n",
      "- Code for coin tossing example\n",
      "- Derivation of update equatiosn for mixture of Gaussians\n",
      "- Code for mixture of Gaussians\n",
      "- More examples to show wide applicability of EM and exercises"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Jensen's inequality\n",
      "----\n",
      "\n",
      "For a convex function $f$, $E[f(x) \\geq f(E[x])$. Flip the signe for a concave function. \n",
      "\n",
      "A function $f(x)$ is convex if $f''(x) \\geq 0$ everywhere in its domain. For example, if $f(x) = \\log x$, $f''(x) = -1/x^2$, so the log function is concave for $x \\in (0, \\infty]$. A visual illustration of Jensen's inequality is shown below.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Image(filename='figs/jensen.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: u'figs/jensen.png'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-3-bda30f78ec52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'figs/jensen.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Users/cliburn/anaconda/lib/python2.7/site-packages/IPython/core/display.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/cliburn/anaconda/lib/python2.7/site-packages/IPython/core/display.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/cliburn/anaconda/lib/python2.7/site-packages/IPython/core/display.pyc\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/cliburn/anaconda/lib/python2.7/site-packages/IPython/core/display.pyc\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: u'figs/jensen.png'"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Derivation of EM\n",
      "----\n",
      "\n",
      "Consider an experiment with coin A that has a probability $\\theta_A$ of heads, and a coin B that has a probability $\\theta_B$ of tails. We draw m sample as follows - for each sample, pick one of the coins, flip it 10 times, and record the number of heads and tails. If we recorded which coin we used for each sample, we have *complete* information and can estimate $\\theta_A$ and $\\theta_B$ in closed form. However, if we did not record the coin we used, we have *missing* data and the problem is harder to solve. EM is an algorihtm for maximum likelikhood optimization when there is missing inforrmaiton. \n",
      "\n",
      "We first start by identifying a function which is a lower bound for the log-likelikelihood\n",
      "\n",
      "\\begin{align}\n",
      "ll &= \\sum_i{\\log p(x^i; \\theta)} && \\text{definition of log likelihood} \\\\\n",
      "&= \\sum_i \\log \\sum_{z^i}{p(x^i, z^i; \\theta)} && \\text{augment with latent variables $z$} \\\\\n",
      "&= \\sum_i \\log \\sum_{z^i} Q(z^i) \\frac{p(x^i, z^i; \\theta)}{Q_i(z^i)} && \\text{$Q_i$ is a distribution for $z^i$} \\\\\n",
      "&= \\sum_i \\log E_{z^i}[\\frac{p(x^i, z^i; \\theta)}{Q_i(z^i)}] && \\text{taking expectations} \\\\\n",
      "&\\geq \\sum E_{z^i}[\\log \\frac{p(x^i, z^i; \\theta)}{Q_i(z^i)}] && \\text{Using Jensen's rule for $\\log x$ which is concave} \\\\\n",
      "&\\geq \\sum_i \\sum_{z^i} Q_i(z^i) \\log \\frac{p(x^i, z^i; \\theta)}{Q_i(z^i)} && \\text{EQN (1) - also known as the Q function}\n",
      "\\end{align}\n",
      "\n",
      "How do we choose the distribution $Q$? We want this function to touch the log-likelihood, and know that Jensen's inequality is an equality only if the function is constant. So\n",
      "\n",
      "\\begin{align}\n",
      "\\frac{p(x^i, z^i; \\theta)}{Q_i(z^i)} =& c \\\\\n",
      "\\implies Q_i(z^i) &\\propto p(x^i, z^i; \\theta)\\\\\n",
      "\\implies Q_i(z^i) &= \\frac{p(x^i, z^i; \\theta) }{\\sum_{z^i}{p(x^i, z^i; \\theta)}} &&\\text{Since $Q$ is a distribution and sums to 1} \\\\\n",
      "\\implies Q_i(z_i) &= \\frac{p(x^i, z^i; \\theta) }{{p(x^i, \\theta)}} && \\text{marginalizing $z^i$}\\\\\n",
      "\\implies Q_i(z^i) &= p(z^i | x^i; \\theta) && \\text{by definition}\n",
      "\\end{align}\n",
      "\n",
      "So $Q_i$ is just the posterior distribution of $z_i$, and this completes the E-step.\n",
      "\n",
      "In the M-step, we find the value of $\\theta$ that maximizes EQA 1, and then we iterate over the E and M steps until convergence. A visual illustration of the two steps in the EM algorithm is shown below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Image from http://www.nature.com/nbt/journal/v26/n8/extref/nbt1406-S1.pdf\n",
      "Image(filename='figs/em.png', width=800)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mn_ll(y, theta, axis=None):\n",
      "    \"\"\"Log likelihood for multinomial distribution (ignoring constant).\"\"\"\n",
      "    return np.sum(y * np.log(theta), axis=axis)\n",
      "    # return np.sum(y * np.log(theta))\n",
      "\n",
      "def ll(y, theta, p):\n",
      "    \"\"\"Complete log likelihood for mixture.\"\"\"\n",
      "    return np.sum(p * mm_ll(y, theta))\n",
      "\n",
      "def normalize(xs, axis=1):\n",
      "    \"\"\"Return normalized marirx so that sum of row or column (default) entries = 1.\"\"\"\n",
      "    if axis==0:\n",
      "        return xs/xs.sum(0)\n",
      "    else:\n",
      "        return xs/xs.sum(1)[:, None]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Coin toss example from [What is the expectation maximization algorithm?](http://www.nature.com/nbt/journal/v26/n8/full/nbt1406.html)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the E-step, we have\n",
      "\n",
      "\\begin{align}\n",
      "w_j &= Q_i(z^i = j) \\\\\n",
      "&= p(z^i = j | x^i; \\theta) \\\\\n",
      "&= \\frac{p(x^i | z^i = j; \\theta) p(z^i = j; \\phi)}  {\\sum_{l=1}^k{p(x^i | z^i = l; \\theta) p(z^i = l; \\phi)}}  && \\text{Baye's rule} \\\\\n",
      "&= \\frac{\\theta_j^h(1-\\theta_j)^{n-h} \\phi_j}{\\sum_{l=1}^k \\theta_l^h(1-\\theta_l)^{n-h} \\phi_l} \\\\\n",
      "&= \\frac{\\theta_j^h(1-\\theta_j)^{n-h} }{\\sum_{l=1}^k \\theta_l^h(1-\\theta_l)^{n-h} } && \\text{assume $\\phi$ is fixed for simplicity}\n",
      "\\end{align}\n",
      "\n",
      "For the M-step, we need to find the value of $\\theta$ that maximises\n",
      "\n",
      "\\begin{align}\n",
      "& \\sum_i \\sum_{z^i} Q_i(z^i) \\log \\frac{p(x^i, z^i; \\theta)}{Q_i(z^i)} \\\\\n",
      "&= \\sum_{i=1}^m \\sum_{j=1}^k w_j \\log \\frac{p(x^i | z^i=j; \\theta)p(z^i = j; \\phi)}{w_j} \\\\\n",
      "&= \\sum_{i=1}^m \\sum_{j=1}^k w_j \\log \\frac{\\theta_j^h(1-\\theta_j)^(n-h) \\phi_j}{w_j} \\\\\n",
      "&= \\sum_{i=1}^m \\sum_{j=1}^k w_j \\left( h \\log \\theta_j + (n-h) \\log (1-\\theta_j) + \\log \\phi_j - \\log w_j \\right)\n",
      "\\end{align}\n",
      "\n",
      "We can differentiate and solve for $\\theta_s$ in the usual way\n",
      "\n",
      "\\begin{align}\n",
      "\\sum_{i=1}^m w_s \\left( \\frac{h}{\\theta_s} - \\frac{n-h}{1-\\theta_s} \\right) &= 0  \\\\\n",
      "\\implies \\theta_s &= \\frac {\\sum_{i=1}^m w_s h}{\\sum_{i=1}^m w_s n}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xs = np.array([(5,5), (9,1), (8,2), (4,6), (7,3)])\n",
      "theta_A, theta_B = np.array([0.6, 0.4]), np.array([0.5, 0.5])\n",
      "\n",
      "tol = 0.01\n",
      "max_iter = 100\n",
      "for i in range(max_iter):\n",
      "    exp_A = []\n",
      "    exp_B = []\n",
      "    ll_new = 0\n",
      "\n",
      "    # E-step: calculate probability distributions over possible completions\n",
      "    for x in xs:\n",
      "\n",
      "        ll_A = mn_ll(x, theta_A)\n",
      "        ll_B = mn_ll(x, theta_B)\n",
      "      \n",
      "        denom = np.exp(ll_A) + np.exp(ll_B)       \n",
      "        w_A = np.exp(ll_A)/denom\n",
      "        w_B = np.exp(ll_B)/denom\n",
      "\n",
      "        exp_A.append(np.dot(w_A, x))\n",
      "        exp_B.append(np.dot(w_B, x))\n",
      "        \n",
      "        # update complete log likelihood\n",
      "        ll_new += w_A * ll_A + w_B * ll_B\n",
      "    \n",
      "    # M-step: update values for parameters given current distribution\n",
      "    theta_A = np.sum(exp_A, 0)/np.sum(exp_A)\n",
      "    theta_B = np.sum(exp_B, 0)/np.sum(exp_B)\n",
      "    # print distribution of z for each x and current parameter estimate\n",
      "    print \"Iteration: %d\" % (i+1)\n",
      "    print \"theta_A = %.2f, theta_B = %.2f, ll = %.2f\" % (thetas[0,0], thetas[1,0], ll_new)\n",
      "\n",
      "    if np.abs(ll_new - ll_old) < tol:\n",
      "        break\n",
      "    ll_old = ll_new"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### After some cleaning up.\n",
      "\n",
      "xs = observed data (sampled from multinomial distribution)\n",
      "wts = conditional distribution of missing data given observed data \n",
      "thetas = parameters to estimate"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xs = np.array([(5,5), (9,1), (8,2), (4,6), (7,3)])\n",
      "# thetas = normalize(np.random.random((2,2)))\n",
      "thetas = np.array([[0.6, 0.4], [0.5, 0.5]])\n",
      "\n",
      "tol = 0.01\n",
      "max_iter = 100\n",
      "for i in range(max_iter):\n",
      "    exp_A = []\n",
      "    exp_B = []\n",
      "    ll_new = 0\n",
      "\n",
      "    # E-step\n",
      "    lls = np.array([mn_ll(x, thetas, axis=1) for x in xs])\n",
      "    ws = normalize(np.exp(lls))\n",
      "        \n",
      "    # M-step\n",
      "    thetas = normalize(np.dot(ws.T, xs))\n",
      "    \n",
      "    # update complete log likelihoood \n",
      "    ll_new = np.sum(np.dot(w, ll) for (w, ll) in zip(ws, lls))\n",
      "    \n",
      "    # print distribution of z for each x and current parameter estimate\n",
      "    print \"Iteration: %d\" % (i+1)\n",
      "    print \"theta_A = %.2f, theta_B = %.2f, ll = %.2f\" % (thetas[0,0], thetas[1,0], ll_new)\n",
      "    print \"Weights\\n\", \\\n",
      "        pd.DataFrame(normalize(ws), columns=['A', 'B'], index=range(1, 1+len(ys))), \"\\n\"\n",
      " \n",
      "    if np.abs(ll_new - ll_old) < tol:\n",
      "        break\n",
      "    ll_old = ll_new"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Gaussian mixture models\n",
      "----\n",
      "\n",
      "A mixture of $k$ Gaussians has the following PDF\n",
      "\n",
      "\\begin{align}\n",
      "p(x) = \\sum_{j=1}^k \\alpha_j \\phi(x; \\mu_j, \\Sigma_j)\n",
      "\\end{align}\n",
      "\n",
      "where $\\alpha_j$ is the weight of the $j^\\text{th}$ Gaussain component and \n",
      "\n",
      "\\begin{align}\n",
      "\\phi(x; \\mu, \\Sigma) = \\frac{1}{(2 \\pi)^{d/2}|\\Sigma|^{1/2}} \\exp \\left( -\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu) \\right)\n",
      "\\end{align}\n",
      "\n",
      "Suppose we observe $y_1, y2, \\ldots, y_n$ as a sample from a mixture of Gaussians. The log-likeihood is then\n",
      "\n",
      "\\begin{align}\n",
      "l(\\theta) = \\sum_{i=1}^n \\log \\left( \\sum_{j=1}^k \\alpha_j \\phi(y_i; \\mu_j, \\Sigma_j) \\right)\n",
      "\\end{align}\n",
      "\n",
      "where $\\theta = (\\alpha, \\mu, \\Sigma)$\n",
      "\n",
      "There is no closed form for maximizing the parameters of this log-likelihood, and it is hard to maximize directly.\n",
      "\n",
      "Using EM\n",
      "----\n",
      "\n",
      "Suppose we augment with the latent variable $z$ that indicates which of the $k$ Gaussians our observation $y$ came from. The derivation of the E and M steps are the same as for the toy example, only with more algebra.\n",
      "\n",
      "For the E-step, we have\n",
      "\n",
      "\\begin{align}\n",
      "w_j^i &= Q_i(z^i = j) \\\\\n",
      "&= p(z^i = j | y^i; \\theta) \\\\\n",
      "&= \\frac{p(y^i | z^i = j; \\mu, \\Sigma) p(z^i = j; \\alpha)}  {\\sum_{l=1}^k{p(y^i | z^i = l; \\mu, \\Sigma) p(z^i = l; \\alpha)}}  && \\text{Baye's rule} \\\\\n",
      "&= \\frac{\\phi(y^i; \\mu_j, \\Sigma_j) \\alpha_j}{\\sum_{l=1}^k \\phi(y^i; \\mu_l, \\Sigma_l) \\alpha_l}\n",
      "\\end{align}\n",
      "\n",
      "For the M-step, we have to find $\\theta = (w, \\mu, \\Sigma)$ that maximizes $Q$\n",
      "\n",
      "\\begin{align}\n",
      "\\sum_{i=1}^{m}\\sum{j=1}^{k} Q(z^i=j) \\log \\frac{p(x^i | z^i= j; \\mu, \\Sigma) p(z^i=j; \\alpha)}{Q(z^i=j)}\n",
      "\\end{align}\n",
      "\n",
      "By taking derivatives with respect to $(w, \\mu, \\Sigma)$ respectively and solving (remember to use Lagrange multipliers for the constraint that $\\sum_{j=1}^k w_j = 1$), we get\n",
      "\n",
      "\\begin{align}\n",
      "\\alpha_j &= \\frac{1}{m} \\sum_{i=1}^{m} w_j^i \\\\\n",
      "\\mu_j &= \\frac{\\sum_{i=1}^{m} w_j^i x^i}{\\sum_{i=1}^{m} w_j^i} \\\\\n",
      "\\Sigma_j &= \\frac{\\sum_{i=1}^{m} w_j^i (x^i - \\mu)(x^i - \\mu)^T}{\\sum_{i1}^{m} w_j^i}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import multivariate_normal as mvn\n",
      "\n",
      "def normalize(xs, axis=None):\n",
      "    \"\"\"Return normalized marirx so that sum of row or column (default) entries = 1.\"\"\"\n",
      "    if axis is None:\n",
      "        return xs/xs.sum()\n",
      "    elif axis==0:\n",
      "        return xs/xs.sum(0)\n",
      "    else:\n",
      "        return xs/xs.sum(1)[:, None]\n",
      "\n",
      "def mix_mvn_pdf(xs, pis, mus, sigmas):\n",
      "    return np.array([pi*mvn(mu, sigma).pdf(xs) for (pi, mu, sigma) in zip(pis, mus, sigmas)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def em_gmm_orig(xs, pis, mus, sigmas, tol=0.01, max_iter=100):\n",
      "\n",
      "    n, p = xs.shape\n",
      "    k = len(pis)\n",
      "\n",
      "    ll_old = 0\n",
      "    for i in range(max_iter):\n",
      "        exp_A = []\n",
      "        exp_B = []\n",
      "        ll_new = 0\n",
      "\n",
      "        # E-step\n",
      "        ws = np.zeros((k, n))\n",
      "        for j in range(len(mus)):\n",
      "            for i in range(n):\n",
      "                ws[j, i] = pis[j] * mvn(mus[j], sigmas[j]).pdf(xs[i])\n",
      "        ws /= ws.sum(0)\n",
      "\n",
      "    #     # vectorized version\n",
      "    #     for j in range(len(mus)):\n",
      "    #         ws[j, :] = pis[j] * mvn(mus[j], sigmas[j]).pdf(xs)\n",
      "    #     ws /= ws.sum(0)\n",
      "\n",
      "        # M-step\n",
      "        pis = np.zeros(k)\n",
      "        for j in range(len(mus)):\n",
      "            for i in range(n):\n",
      "                pis[j] += ws[j, i]\n",
      "        pis /= n\n",
      "\n",
      "    #     # vectorization 1\n",
      "    #     pis = np.zeros(len(mus))\n",
      "    #     for j in range(len(mus)):\n",
      "    #         pis[j] = ws[j,:].sum()\n",
      "    #     pis /= m\n",
      "\n",
      "    #     # vectorization 2\n",
      "    #     pis = ws.sum(axis=1)\n",
      "    #     pis /= m\n",
      "\n",
      "        mus = np.zeros((k, p))\n",
      "        for j in range(k):\n",
      "            for i in range(n):\n",
      "                mus[j] += ws[j, i] * xs[i]\n",
      "            mus[j] /= ws[j, :].sum()\n",
      "\n",
      "    #     # vectorization 1\n",
      "    #     mus = np.zeros((len(mus), p))\n",
      "    #     for j in range(len(mus)):\n",
      "    #         for k in range(m):\n",
      "    #             mus[j] += ws[j, k] * xs[k]\n",
      "    #     mus /= ws.sum(axis=1)[:, None]\n",
      "\n",
      "    #     # Vectorization 2\n",
      "    #     mus = np.zeros((len(mus), p))\n",
      "    #     for j in range(len(mus)):\n",
      "    #         mus[j, :] = np.dot(ws[j, :], xs)\n",
      "    #         mus[j] /= ws[j, :].sum()\n",
      "\n",
      "    #     # Vectorization 3\n",
      "    #     mus = np.dot(ws, xs)\n",
      "    #     mus /= ws.sum(1)[:, None]\n",
      "\n",
      "        sigmas = np.zeros((k, p, p))\n",
      "        for j in range(k):\n",
      "            for i in range(n):\n",
      "                ys = np.reshape(xs[i]- mus[j], (2,1))\n",
      "                sigmas[j] += ws[j, i] * np.dot(ys, ys.T)\n",
      "            sigmas[j] /= ws[j,:].sum()\n",
      "\n",
      "    #     # Vectorization 1\n",
      "    #     sigmas = np.zeros((len(mus), p, p))\n",
      "    #     for j in range(len(mus)):\n",
      "    #         ys = xs - mus[j, :]\n",
      "    #         sigmas[j] = (ws[j,:,None,None] * mm(ys[:,:,None], ys[:,None,:])).sum(axis=0)\n",
      "    #     sigmas /= ws.sum(axis=1)[:,None,None]\n",
      "\n",
      "        # update complete log likelihoood \n",
      "        ll_new = 0.0\n",
      "        for i in range(n):\n",
      "            s = 0\n",
      "            for j in range(k):\n",
      "                s += pis[j] * mvn(mus[j], sigmas[j]).pdf(xs[i])\n",
      "            ll_new += np.log(s)\n",
      "\n",
      "    #     # Vectorization 1\n",
      "    #     ll_new = 0\n",
      "    #     for pi, mu, sigma in zip(pis, mus, sigmas):\n",
      "    #         ll_new += pi*mvn(mu, sigma).pdf(xs)\n",
      "    #     ll_new = np.log(ll_new).sum()\n",
      "\n",
      "        if np.abs(ll_new - ll_old) < tol:\n",
      "            break\n",
      "        ll_old = ll_new\n",
      "\n",
      "    return ll_new, pis, mus, sigmas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vectorized version\n",
      "----"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def em_gmm_vect(xs, pis, mus, sigmas, tol=0.01, max_iter=100):\n",
      "\n",
      "    n, p = xs.shape\n",
      "    k = len(pis)\n",
      "\n",
      "    ll_old = 0\n",
      "    for i in range(max_iter):\n",
      "        exp_A = []\n",
      "        exp_B = []\n",
      "        ll_new = 0\n",
      "\n",
      "        # E-step\n",
      "        ws = np.zeros((k, n))\n",
      "        for j in range(k):\n",
      "            ws[j, :] = pis[j] * mvn(mus[j], sigmas[j]).pdf(xs)\n",
      "        ws /= ws.sum(0)\n",
      "\n",
      "        # M-step\n",
      "        pis = ws.sum(axis=1)\n",
      "        pis /= n\n",
      "\n",
      "        mus = np.dot(ws, xs)\n",
      "        mus /= ws.sum(1)[:, None]\n",
      "\n",
      "        sigmas = np.zeros((k, p, p))\n",
      "        for j in range(k):\n",
      "            ys = xs - mus[j, :]\n",
      "            sigmas[j] = (ws[j,:,None,None] * mm(ys[:,:,None], ys[:,None,:])).sum(axis=0)\n",
      "        sigmas /= ws.sum(axis=1)[:,None,None]\n",
      "\n",
      "        # update complete log likelihoood \n",
      "        ll_new = 0\n",
      "        for pi, mu, sigma in zip(pis, mus, sigmas):\n",
      "            ll_new += pi*mvn(mu, sigma).pdf(xs)\n",
      "        ll_new = np.log(ll_new).sum()\n",
      "\n",
      "        if np.abs(ll_new - ll_old) < tol:\n",
      "            break\n",
      "        ll_old = ll_new\n",
      "\n",
      "    return ll_new, pis, mus, sigmas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vectorization with Einstein summation notation\n",
      "----"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def em_gmm_eins(xs, pis, mus, sigmas, tol=0.01, max_iter=100):\n",
      "\n",
      "    n, p = xs.shape\n",
      "    k = len(pis)\n",
      "\n",
      "    ll_old = 0\n",
      "    for i in range(max_iter):\n",
      "        exp_A = []\n",
      "        exp_B = []\n",
      "        ll_new = 0\n",
      "\n",
      "        # E-step\n",
      "        ws = np.zeros((k, n))\n",
      "        for j, (pi, mu, sigma) in enumerate(zip(pis, mus, sigmas)):\n",
      "            ws[j, :] = pi * mvn(mu, sigma).pdf(xs)\n",
      "        ws /= ws.sum(0)\n",
      "\n",
      "        # M-step\n",
      "        pis = np.einsum('kn->k', ws)/n\n",
      "        mus = np.einsum('kn,np -> kp', ws, xs)/ws.sum(1)[:, None]\n",
      "        sigmas = np.einsum('kn,knp,knq -> kpq', ws, \n",
      "            xs-mus[:,None,:], xs-mus[:,None,:])/ws.sum(axis=1)[:,None,None]\n",
      "\n",
      "        # update complete log likelihoood \n",
      "        ll_new = 0\n",
      "        for pi, mu, sigma in zip(pis, mus, sigmas):\n",
      "            ll_new += pi*mvn(mu, sigma).pdf(xs)\n",
      "        ll_new = np.log(ll_new).sum()\n",
      "\n",
      "        if np.abs(ll_new - ll_old) < tol:\n",
      "            break\n",
      "        ll_old = ll_new\n",
      "\n",
      "    return ll_new, pis, mus, sigmas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Comparison of EM routines\n",
      "----"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(123)\n",
      "\n",
      "# create data set\n",
      "n = 1000\n",
      "_mus = np.array([[0,4], [-2,0]])\n",
      "_sigmas = np.array([[[3, 0], [0, 0.5]], [[1,0],[0,2]]])\n",
      "_pis = np.array([0.6, 0.4])\n",
      "xs = np.concatenate([np.random.multivariate_normal(mu, sigma, int(pi*n)) \n",
      "                    for pi, mu, sigma in zip(_pis, _mus, _sigmas)])\n",
      "\n",
      "# initial guesses for parameters\n",
      "pis = normalize(np.random.random(2))\n",
      "mus = np.random.random((2,2))\n",
      "sigmas = np.array([np.eye(2)] * 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "ll1, pis1, mus1, sigmas1 = em_gmm_orig(xs, pis, mus, sigmas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "intervals = 101\n",
      "ys = np.linspace(-8,8,intervals)\n",
      "X, Y = np.meshgrid(ys, ys)\n",
      "_ys = np.vstack([X.ravel(), Y.ravel()]).T\n",
      "\n",
      "z = np.zeros(len(_ys))\n",
      "for pi, mu, sigma in zip(pis1, mus1, sigmas1):\n",
      "    z += pi*mvn(mu, sigma).pdf(_ys)\n",
      "z = z.reshape((intervals, intervals))\n",
      "\n",
      "ax = plt.subplot(111)\n",
      "plt.scatter(xs[:,0], xs[:,1], alpha=0.2)\n",
      "plt.contour(X, Y, z, N=10)\n",
      "plt.axis([-8,6,-6,8])\n",
      "ax.axes.set_aspect('equal')\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "ll2, pis2, mus2, sigmas2 = em_gmm_vect(xs, pis, mus, sigmas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "intervals = 101\n",
      "ys = np.linspace(-8,8,intervals)\n",
      "X, Y = np.meshgrid(ys, ys)\n",
      "_ys = np.vstack([X.ravel(), Y.ravel()]).T\n",
      "\n",
      "z = np.zeros(len(_ys))\n",
      "for pi, mu, sigma in zip(pis2, mus2, sigmas2):\n",
      "    z += pi*mvn(mu, sigma).pdf(_ys)\n",
      "z = z.reshape((intervals, intervals))\n",
      "\n",
      "ax = plt.subplot(111)\n",
      "plt.scatter(xs[:,0], xs[:,1], alpha=0.2)\n",
      "plt.contour(X, Y, z, N=10)\n",
      "plt.axis([-8,6,-6,8])\n",
      "ax.axes.set_aspect('equal')\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "ll3, pis3, mus3, sigmas3 = em_gmm_eins(xs, pis, mus, sigmas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# %timeit em_gmm_orig(xs, pis, mus, sigmas)\n",
      "%timeit em_gmm_vect(xs, pis, mus, sigmas)\n",
      "%timeit em_gmm_eins(xs, pis, mus, sigmas)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}