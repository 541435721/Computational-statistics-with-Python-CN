{
 "metadata": {
  "name": "",
  "signature": "sha256:c60d2882d61cc783f4a60d702e8b783244958e9221413ff5264bcaf934fa96c1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Linear Algebra and Linear Systems"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A lot of problems in statistical computing can be described mathematically using linear algebra.  This lecture is meant to serve as a review of concepts you have covered in linear algebra courses."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Simultaneous Equations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider a set of $m$ linear equations in $n$ unknowns:\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\\begin{align*}\n",
      "a_{11} x_1 + &a_{12} x_2& +& ... + &a_{1n} x_n &=& b_1\\\\\n",
      "\\vdots  && &&\\vdots &= &\\vdots\\\\\n",
      "a_{m1} x_1 + &a_{m2} x_2& +& ... + &a_{mn} x_n &=&b_m \n",
      "\\end{align*}\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "We can let:\n",
      "\n",
      "\\begin{align*}\n",
      "    A=\\left[\\begin{matrix}a_{11}&\\cdots&a_{1n}\\\\\n",
      "               \\vdots & &\\vdots\\\\\n",
      "               a_{m1}&\\cdots&a_{mn}\\end{matrix}\\right], & & \n",
      "\n",
      "x = \\left[\\begin{matrix}x_1\\\\\n",
      "               \\vdots\\\\\n",
      "               x_n\\end{matrix}\\right] & \\;\\;\\;\\;\\textrm{   and } &\n",
      "b =  \\left[\\begin{matrix}b_1\\\\\n",
      "               \\vdots\\\\\n",
      "               b_m\\end{matrix}\\right]\n",
      "\\end{align*}\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "We can let:\n",
        "\n",
        "\\begin{align*}\n",
        "    A=\\left[\\begin{matrix}a_{11}&\\cdots&a_{1n}\\\\\n",
        "               \\vdots & &\\vdots\\\\\n",
        "               a_{m1}&\\cdots&a_{mn}\\end{matrix}\\right], & & \n",
        "\n",
        "x = \\left[\\begin{matrix}x_1\\\\\n",
        "               \\vdots\\\\\n",
        "               x_n\\end{matrix}\\right] & \\;\\;\\;\\;\\textrm{   and } &\n",
        "b =  \\left[\\begin{matrix}b_1\\\\\n",
        "               \\vdots\\\\\n",
        "               b_m\\end{matrix}\\right]\n",
        "\\end{align*}\n"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x7f20d85e0390>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "And re-write the system:\n",
      "    \n",
      "$$ Ax = b$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Solving the system now amounts to finding $A^{-1}$.  Of course, strictly speaking, $A^{-1}$ is defined only when $A$ is an $n\\times n$ matrix of full rank.  There are weaker forms of inverse (generalized inverses) that work in the case that $A$ is $m\\times n$.  A generalized inverse of $A$, $A^g$, is a matrix with the property:  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$A A^g A = A$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The generalized inverse is not unique.  When it exists, the Moore-Penrose inverse is useful.  It has the additional properties:\n",
      "\n",
      "$$A^g A A^g = A^g$$\n",
      "$$\\left(A A^g\\right)^t = A A^g$$\n",
      "$$\\left(A^g A\\right)^t = A^g A$$\n",
      "\n",
      "In other words, $A$ is also a generalized inverse of $A^g$, and the products $A A^g$ and $A^g A$ are symmetric."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If $A^g$ is the Moore-Penrose inverse of $A$, then \n",
      "$$x = A^g b$$\n",
      "is the shortest length least-squares solution to\n",
      "$$Ax =b$$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, lets solve a system in Python!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "np.set_printoptions(suppress=True)   # Suppress printing of negligibly small numbers - print as zero.\n",
      "a = np.random.randn(6, 9)\n",
      "a_pseudo_inv = np.linalg.pinv(a)\n",
      "print(np.dot(a,a_pseudo_inv))                  # We can see that a_pseudo_inv has inverted part of a.\n",
      "\n",
      "b = np.random.rand(6,1)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1. -0.  0. -0. -0.  0.]\n",
        " [ 0.  1.  0.  0.  0. -0.]\n",
        " [ 0. -0.  1. -0.  0.  0.]\n",
        " [-0. -0.  0.  1. -0.  0.]\n",
        " [-0.  0.  0. -0.  1.  0.]\n",
        " [-0.  0.  0.  0. -0.  1.]]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The solution to the system $Ax=b$ is given by:\n",
      "    $$A^gb +(I - A^gA)v$$\n",
      "where $v$ is an arbitrary vector.  If $A$ is invertible, then $A^g$ is its unique inverse and $I-A^gA$ is the zero matrix."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(np.dot(a_pseudo_inv,b)) \n",
      "print(np.identity(9) - np.dot(a_pseudo_inv, a))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.38711309]\n",
        " [-0.31442491]\n",
        " [ 0.66029409]\n",
        " [-0.0751685 ]\n",
        " [-0.45190989]\n",
        " [ 0.26597446]\n",
        " [-0.11829717]\n",
        " [-0.2353295 ]\n",
        " [ 0.32334829]]\n",
        "[[ 0.16300068  0.04941738  0.11682394 -0.03435597 -0.01904584 -0.25152876\n",
        "   0.04311034 -0.04165154 -0.22789999]\n",
        " [ 0.04941738  0.28918131  0.19460042  0.25853562 -0.11147669  0.09495047\n",
        "   0.06234735  0.20278236 -0.17875372]\n",
        " [ 0.11682394  0.19460042  0.17969467  0.09655198 -0.0845402  -0.0855338\n",
        "   0.04460437  0.13971148 -0.22492916]\n",
        " [-0.03435597  0.25853562  0.09655198  0.61488229 -0.00597979  0.26670732\n",
        "   0.18579339 -0.21768886 -0.07989118]\n",
        " [-0.01904584 -0.11147669 -0.0845402  -0.00597979  0.06849969 -0.02414093\n",
        "   0.01589582 -0.19744794  0.063354  ]\n",
        " [-0.25152876  0.09495047 -0.0855338   0.26670732 -0.02414093  0.50112816\n",
        "  -0.01625195  0.14051147  0.28049458]\n",
        " [ 0.04311034  0.06234735  0.04460437  0.18579339  0.01589582 -0.01625195\n",
        "   0.08271626 -0.15887773 -0.0886618 ]\n",
        " [-0.04165154  0.20278236  0.13971148 -0.21768886 -0.19744794  0.14051147\n",
        "  -0.15887773  0.73719503 -0.00198708]\n",
        " [-0.22789999 -0.17875372 -0.22492916 -0.07989118  0.063354    0.28049458\n",
        "  -0.0886618  -0.00198708  0.36370191]]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are many techniques to solve linear systems.  Our goal is to understand the theory behind many of the built-in functions, and how they *efficiently* solve systems of equations.  In fact, finding $A^{-1}$ is often *not* the best approach to solving a system of equations.\n",
      "\n",
      "\n",
      "First, let's review some linear algebra topics:"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Column space, Row space, Rank and Kernel"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let $A$ be an $m\\times n$ matrix.  We can view the columns of $A$ as vectors, say $\\textbf{a_1},...,\\textbf{a_n}$. The space of all linear combinations of the $\\textbf{a_i}$ are the *column space* of the matrix $A$.  Now, if $\\textbf{a_1},...,\\textbf{a_n}$ are *linearly independent*, then the column space is of dimension $n$.  Otherwise, the dimension of the column space is the size of the maximal set of linearly independent $\\textbf{a_i}$.  Row space is exactly analogous, but the vectors are the *rows* of $A$.\n",
      "\n",
      "The *rank* of a matrix *A* is the dimension of its column space - and - the dimension of its row space.  These are equal for any matrix.  Rank can be thought of as a measure of non-degeneracy of a system of linear equations, in that it is the dimension of the image of the linear transformation determined by $A$. \n",
      "\n",
      "The *kernel* of a matrix *A* is the dimension of the space mapped to zero under the linear transformation that $A$ represents. The dimension of the kernel of a linear transformation is called the *nullity*. \n",
      "\n",
      "Index theorem: For and $m\\times n$ matrix $A$, rank($A$) + nullity($A$) = $n$.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Special Matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some matrices have interesting properties that allow us either simplify the underlying linear system or to understand more about it. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Square Matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Square matrices have the same number of columns (usually denoted $n$).  We refer to an arbitrary square matrix as and $n\\times n$ or we refer to it as a 'square matrix of dimension $n$'.  If an $n\\times n$ matrix $A$ has *full rank* (i.e. it has rank $n$), then $A$ is invertible, and its inverse is unique.  This is a situation that leads to a unique solution to a linear system."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Diagonal Matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A diagonal matrix is a matrix with all entries off the diagonal equal to zero.  Strictly speaking, such a matrix should be square, but we can also consider rectangular matrices of size $m\\times n$ to be diagonal, if all entries $a_{ij}$ are zero for $i\\neq j$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Symmetric and Skew Symmetric"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A matrix $A$ is (skew) symmetric if $a_{ij} = (-)a_{ji}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Upper and Lower Triangular"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A matrix $A$ is (upper|lower) triangular if $a_{ij} = 0$ for all $i (>|<) j$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Matrix Decompositions for Square Matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Matrix decompositions are an important step in solving linear systems in a computationally efficient manner. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "LU Decomposition and Gaussian Elimination"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "LU stands for 'Lower Upper', and so an LU decomposition of a matrix $A$ is a decomposition so that \n",
      "$$A= LU$$\n",
      "where $L$ is lower triangular and $U$ is upper triangular.\n",
      "\n",
      "Now, LU decomposition is essentially gaussian elimination, with a small difference that helps when solution for multiple right-hand-side vectors $b$ is required.\n",
      "\n",
      "Let's review how gaussian elimination (ge) works.  We will deal with a $3\\times 3$ system of equations for conciseness, but everything here generalizes to the $n\\times n$ case. Consider the following equation:\n",
      "\n",
      "$$\\left(\\begin{matrix}a_{11}&a_{12} & a_{13}\\\\a_{21}&a_{22}&a_{23}\\\\a_{31}&a_{32}&a_{33}\\end{matrix}\\right)\\left(\\begin{matrix}x_1\\\\x_2\\\\x_3\\end{matrix}\\right) = \\left(\\begin{matrix}b_1\\\\b_2\\\\b_3\\end{matrix}\\right)$$\n",
      "\n",
      "For simplicity, let us assume that the leftmost matrix $A$ is non-singular.  To solve the system using ge, we start with the 'augmented matrix':\n",
      "\n",
      "$$\\left(\\begin{array}{ccc|c}a_{11}&a_{12} & a_{13}& b_1 \\\\a_{21}&a_{22}&a_{23}&b_2\\\\a_{31}&a_{32}&a_{33}&b_3\\end{array}\\right)$$\n",
      "\n",
      "We begin at the first entry, $a_{11}$.  If $a_{11} \\neq 0$, then we divide the first row by $a_{11}$ and then subtract the appropriate multiple of the first row from each of the other rows, zeroing out the first entry of all rows. (If $a_{11}$ is zero, we need to permute rows.  We will not go into detail of that here.)  The result is as follows:\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\left(\\begin{array}{ccc|c}\n",
      "1 & \\frac{a_{12}}{a_{11}} & \\frac{a_{13}}{a_{11}} & \\frac{b_1}{a_{11}} \\\\\n",
      "0 & a_{22} - a_{21}\\frac{a_{12}}{a_{11}} & a_{23} - a_{21}\\frac{a_{13}}{a_{11}}  & b_2 - a_{21}\\frac{b_1}{a_{11}}\\\\\n",
      "0&a_{32}-a_{31}\\frac{a_{12}}{a_{11}} & a_{33} - a_{31}\\frac{a_{13}}{a_{11}}  &b_3- a_{31}\\frac{b_1}{a_{11}}\\end{array}\\right)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We repeat the procedure for the second row, first dividing by the leading entry, then subtracting the appropriate multiple of the resulting row from each of the third and first rows, so that the second entry in row 1 and in row 3 are zero.  We *could* continue until the matrix on the left is the identity. In that case, we can then just 'read off' the solution: i.e., the vector $x$ is the resulting column vector on the right.  Usually, it is more efficient to stop at *reduced row eschelon* form (upper triangular, with ones on the diagonal), and then use *back substitution* to obtain the final answer.\n",
      "\n",
      "Note that in some cases, it is necessary to permute rows to obtain reduced row eschelon form.  This is called *partial pivoting*.  If we also manipulate columns, that is called *full pivoting*.\n",
      "\n",
      "It should be mentioned that we may obtain the inverse of a matrix using ge, by reducing the matrix $A$ to the identity, with the identity matrix as the augmented portion.  \n",
      "\n",
      "Now, this is all fine when we are solving a system one time, for one outcome $b$.  Many applications involve solutions to multiple problems, where the left-hand-side of our matrix equation does not change, but there are many outcome vectors $b$.  In this case, it is more efficient to decompose $A$.\n",
      "\n",
      "First, we start just as in ge, but we 'keep track' of the various multiples required to eliminate entries.  For example, consider the matrix\n",
      "\n",
      "$$A = \\left(\\begin{matrix} 1 & 3 & 4 \\\\\n",
      "                           2& 1& 3\\\\\n",
      "                           4&1&2\n",
      "                           \\end{matrix}\\right)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to multiply row $1$ by $2$ and subtract from row $2$ to eliminate the first entry in row $2$, and then multiply row $1$ by $4$ and subtract from row $3$. Instead of entering zeroes into the first entries of rows $2$ and $3$, we record the multiples required for their elimination, as so:\n",
      "\n",
      "$$\\left(\\begin{matrix} 1 & 3 & 4 \\\\\n",
      "                           (2)& -5 & -5\\\\\n",
      "                           (4)&-11&-14\n",
      "                           \\end{matrix}\\right)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "And then we eliminate the second entry in the third row:\n",
      "\n",
      "\n",
      "$$\\left(\\begin{matrix} 1 & 3 & 4 \\\\\n",
      "                           (2)& -5 & -5\\\\\n",
      "                           (4)&(\\frac{-11}{5})&-3\n",
      "                           \\end{matrix}\\right)$$\n",
      "                           \n",
      "And now we have the decomposition:\n",
      "$$L= \\left(\\begin{matrix} 1 & 0 & 0 \\\\\n",
      "                           2& 1 & 0\\\\\n",
      "                           4&\\frac{-11}5&1\n",
      "                           \\end{matrix}\\right)\n",
      "                          U = \\left(\\begin{matrix} 1 & 3 & 4 \\\\\n",
      "                           0& -5 & -5\\\\\n",
      "                           0&0&-3\n",
      "                           \\end{matrix}\\right)$$\n",
      "                                                  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can solve the system by solving two back-substitution problems:\n",
      "    \n",
      "$$Ly = b$$    and\n",
      "$$Ux=y$$\n",
      "\n",
      "\n",
      "These are both $O(n^2)$, so it is more efficient to decompose when there are multiple outcomes to solve for."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Cholesky Decomposition"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Positive Definite Matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Positive definite matrices are an important class of matrices with very desirable properties. A square matrix $A$ is positive definite if\n",
      "\n",
      "$$u^TA u > 0$$\n",
      "\n",
      "for any non-zero n-dimensional vector $u$.\n",
      "\n",
      "A symmetric, positive-definite matrix $A$ is a positive-definite matrix such that\n",
      "\n",
      "$$A = A^T$$\n",
      "\n",
      "IMPORTANT: \n",
      "\n",
      "* Symmetric, positive-definite matrices have 'square-roots' (in a sense)\n",
      "* Any symmetric, positive-definite matrix is *diagonizable*!!!\n",
      "* Co-variance matrices are symmetric and positive-definite\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Cholesky Decompostion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let $A$ be a symmetric, positive-definite matrix.  There is a unique decomposition such that\n",
      "\n",
      "$$A = L L^T$$\n",
      "\n",
      "where $L$ is lower-triangular with positive diagonal elements and $L^T$ is its transpose.  This decomposition is known as the Cholesky decompostion, and $L$ may be interpreted as the 'square root' of the matrix $A$.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Algorithm:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let $A$ be an $n\\times n$ matrix.  We find the matri $L$ using the following iterative procedure:\n",
      "\n",
      "\n",
      "$$A = \\left(\\begin{matrix}a_{11}&A_{12}\\\\A_{12}&A_{22}\\end{matrix}\\right) =\n",
      "\\left(\\begin{matrix}\\ell_{11}&0\\\\\n",
      "L_{12}&L_{22}\\end{matrix}\\right)\n",
      "\\left(\\begin{matrix}\\ell_{11}&L_{12}\\\\0&L_{22}\\end{matrix}\\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1.) Let $\\ell_{11} = \\sqrt{a_{11}}$\n",
      "\n",
      "2.) $L_{12} = \\frac{1}{\\ell_{11}}A_{12}$\n",
      "\n",
      "3.) Solve $A_{22} - L_{12}L_{12}^T = L_{22}L_{22}^T$ for $L_{22}$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Example:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$A = \\left(\\begin{matrix}1&3&5\\\\3&13&23\\\\5&23&42\\end{matrix}\\right)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\ell_{11} = \\sqrt{a_11} = 1$$\n",
      "\n",
      "$$L_{12} = \\frac{1}{\\ell_{11}} A_{12} = A_{12}$$\n",
      "\n",
      "$\\begin{eqnarray*}\n",
      "A_22 - L_{12}L_{12}^T &=& \\left(\\begin{matrix}13&23\\\\23&42\\end{matrix}\\right) - \\left(\\begin{matrix}9&15\\\\15&25\\end{matrix}\\right)\\\\\n",
      "&=& \\left(\\begin{matrix}4&8\\\\8&17\\end{matrix}\\right)\\\\\n",
      "&=& \\left(\\begin{matrix}2&0\\\\4&\\ell_{33}\\end{matrix}\\right) \\left(\\begin{matrix}2&4\\\\0&\\ell_{33}\\end{matrix}\\right)\\\\\n",
      "&=& \\left(\\begin{matrix}4&8\\\\8&16+\\ell_{33}^2\\end{matrix}\\right)\n",
      "\\end{eqnarray*}$\n",
      "\n",
      "And so we conclude that $\\ell_{33}=1$.\n",
      "\n",
      "\n",
      "This yields the decomposition:\n",
      "\n",
      "\n",
      "$$\\left(\\begin{matrix}1&3&5\\\\3&13&23\\\\5&23&42\\end{matrix}\\right) = \n",
      "\\left(\\begin{matrix}1&0&0\\\\3&2&0\\\\5&4&1\\end{matrix}\\right)\\left(\\begin{matrix}1&3&5\\\\0&2&4\\\\0&0&1\\end{matrix}\\right)$$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Matrix Decompositions for Rectangular Matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this section we will review some decompositions that are valid for non-square, $m\\times n$ matrices."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Eigendecomposition"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Eigenvectors and Eigenvalues"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First recall that an *eigenvector* of a matrix $A$ is a non-zero vector $v$ such that\n",
      "\n",
      "$$Av = \\lambda v$$\n",
      "\n",
      "for some scalar $\\lambda$\n",
      "\n",
      "The value $\\lambda$ is called an *eigenvalue* of $A$.\n",
      "\n",
      "If an $n\\times n$ matrix $A$ has $n$ linearly independent eigenvectors, then $A$ may be decomposed in the following manner:\n",
      "\n",
      "$$A = B\\Lambda B^{-1}$$\n",
      "\n",
      "where $\\Lambda$ is a diagonal matrix whose diagonal entries are the eigenvalues of $A$ and the columns of $B$ are the corresponding eigenvectors of $A$.\n",
      "\n",
      "Facts: \n",
      "\n",
      "* An $n\\times n$ matrix is diagonizable $\\iff$ it has $n$ linearly independent eigenvectors.\n",
      "* A symmetric, positive definite matrix has only positive eigenvalues and its eigendecomposition \n",
      "$$A=B\\Lambda B^{-1}$$\n",
      "\n",
      "is via an orthogonal transformation $B$. (I.e. its eigenvectors are an orthonormal set)\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Calculating Eigenvalues"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is easy to see from the definition that if $v$ is an eigenvector of an $n\\times n$ matrix $A$ with eigenvalue $\\lambda$, then\n",
      "\n",
      "$$Av - \\lambda I = \\bf{0}$$\n",
      "\n",
      "where $I$ is the identity matrix of dimension $n$ and $\\bf{0}$ is an n-dimensional zero vector. Therefore, the eigenvalues of $A$ satisfy:\n",
      "\n",
      "$$\\det\\left(A-\\lambda I\\right)=0$$\n",
      "\n",
      "The left-hand side above is a polynomial in $\\lambda$, and is called the *characteristic polynomial* of $A$.  Thus, to find the eigenvalues of $A$, we find the roots of the characteristic polynomial.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Computationally, however, computing the characteristic polynomial and then solving for the roots is prohibitively expensive.  Therefore, in practice, numerical methods are used - both to find eigenvalues and their corresponding eigenvectors."
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "QR decompositon"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As with the previous decompositions, $QR$ decomposition is a method to write a matrix $A$ as the product of two matrices of simpler form.  In this case, we want:\n",
      "\n",
      "$$ A= QR$$\n",
      "where $Q$ is an $m\\times n$ matrix with $Q Q^T = I$ (i.e. $Q$ is *orthogonal* and $R$ is an $n\\times n$ upper-triangular matrix.\n",
      "\n",
      "This is really just the matrix form of the Grahm-Schmidt orthogonalization of the columns of $A$.  The G-S algorithm itself is unstable, so various other methods have been developed to compute the QR decomposition."
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Householder Method"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Singular Value Decomposition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another important matrix decomposition is singular value decomposition or SVD.   For any $m\\times n$ matrix $A$, we may write:\n",
      "\n",
      "$$A= UDV$$\n",
      "\n",
      "where $U$ is a unitary (orthogonal in the real case) $m\\times m$ matrix, $D$ is a rectangular, diagonal $m\\times n$ matrix with diagonal entries $d_1,...,d_m$ all non-negative. $V$ is a unitary (orthogonal) $n\\times n$ matrix. SVD is used in principle component analysis and in the computation of the Moore-Penrose pseudo-inverse used above."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Condition Number"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}